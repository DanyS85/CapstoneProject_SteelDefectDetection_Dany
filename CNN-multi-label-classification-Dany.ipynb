{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3f0ace-fdc0-4585-aea1-49ac84681ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import segmentation_models as sm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import time\n",
    "import glob\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "from scipy import io\n",
    "import tensorflow_datasets as tfds\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from skimage import color\n",
    "from skimage.transform import resize\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from skimage import data, exposure\n",
    "import seaborn as sns\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c13e08-7b35-4498-9a71-4cf48ee67f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in and prepare augmented images for train\n",
    "\n",
    "augmented_images =[]\n",
    "augmented_labels = []\n",
    "IMG_SIZE = 299\n",
    "for class_id in [1,2,3,4]:\n",
    "    suffix = 'c' + str(class_id)+ '/'\n",
    "    directory_path = glob.glob('data/segmentation/train_aug/' + suffix)[0]\n",
    "    for img_path in sorted(glob.glob(os.path.join(directory_path, \"*.jpg\"))):\n",
    "        image_id = img_path.split('/')[-1]\n",
    "        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        #resize images\n",
    "        image_resized=cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
    "        #print(np.array(image_resized).shape)\n",
    "       \n",
    "        #The input data have to be converted from 3 dimensional format to 1 dimensional format\n",
    "#        image_flat = image_resized.reshape(1, 3*IMG_SIZE*IMG_SIZE)\n",
    "        # Data Normalization\n",
    "        # Conversion to float\n",
    "#        image_flat=image_flat.astype('float32')\n",
    "        # Normalization (In the RGB color space the red, green and blue have integer values from 0 to 255)\n",
    "        image_resized = image_resized/255.0\n",
    "        augmented_images.append(image_resized)\n",
    "        augmented_labels.append(class_id)\n",
    "  #      print(np.array(augmented_images).shape)\n",
    "   #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9bf57b-0f76-48cd-8b05-9485a900a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = augmented_images\n",
    "y_train = augmented_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915dcf43-2bfc-4db0-a88c-15457367d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.Series(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7741eb-2a53-481f-8785-7d18267f2fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0939343-3255-4355-814f-1f73e15937a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12abb846-720f-4b93-9b2c-cc658f781f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, 'Python_Scripts')\n",
    "\n",
    "import util\n",
    "df = pd.read_csv('data/train_complete.csv')\n",
    "util.isolate_single_defects(df)\n",
    "df2 = df.query('ClassId != 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e972e5-dab7-4510-91c3-e446a712ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_images =[]\n",
    "\n",
    "IMG_SIZE = 128\n",
    "for image_id in sorted(df2['ImageId']):\n",
    "    image = io.imread('data/single_defect_train_images/' + image_id)\n",
    "    #resize images\n",
    "    image_resized=cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
    "    #The input data have to be converted from 3 dimensional format to 1 dimensional format\n",
    "    image_flat = image_resized.reshape(1, 3*IMG_SIZE*IMG_SIZE)\n",
    "    # Data Normalization\n",
    "    # Conversion to float\n",
    "    image_flat=image_flat.astype('float32')\n",
    "    # Normalization (In the RGB color space the red, green and blue have integer values from 0 to 255)\n",
    "    image_flat = image_flat/255.0\n",
    "    initial_images.append([image_id,image_flat[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecad656-765d-4165-95bf-4b66d4054e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_images = pd.DataFrame(initial_images , columns = ['ImageId', 'flattened_images'])\n",
    "split_initial_images = pd.DataFrame(initial_images['flattened_images'].tolist())\n",
    "initial_images_complete = pd.concat([initial_images, split_initial_images], axis=1)\n",
    "initial_images_complete.drop(['flattened_images'], axis=1, inplace=True)\n",
    "initial_images_complete = pd.merge(initial_images_complete, df[['ImageId','ClassId']], on='ImageId')\n",
    "y = initial_images_complete['ClassId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d768d9-4db3-4450-b819-be79afd9f7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y = pd.Series(y)\n",
    "y = pd.Series(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36157638-e031-4109-8e7f-1417dabc40d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f33391-f9b9-4d7a-be5a-d6def6896b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "images =[]\n",
    "\n",
    "IMG_SIZE = 299\n",
    "for image_id in sorted(df2['ImageId']):\n",
    "    image = cv2.imread('data/single_defect_train_images/' + image_id,cv2.IMREAD_COLOR)\n",
    "    #resize images\n",
    "    image_resized=cv2.resize(image,(IMG_SIZE,IMG_SIZE))\n",
    "    #The input data have to be converted from 3 dimensional format to 1 dimensional format\n",
    "  #  image_flat = image_resized.reshape(1, 3*IMG_SIZE*IMG_SIZE)\n",
    "    # Data Normalization\n",
    "    # Conversion to float\n",
    " #   image_flat=image_flat.astype('float32')\n",
    "    # Normalization (In the RGB color space the red, green and blue have integer values from 0 to 255)\n",
    "    image_resized = image_resized/255.0\n",
    "  #  images.append([image_id,image_resized[0]])\n",
    "    images.append(image_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822a2d17-a5fa-4884-b9b1-fd6b9cd52b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_val = images[1:2000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b667ce4-e034-41d0-8667-8b9ae95adcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val = y[1:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b112f4be-e1e9-4821-b58f-f211ef63820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebb6947-44ea-461f-bd17-d7e21d5bbf78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_val = np.array(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4fbf95-2899-41f1-817c-0c3fbfa41b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.applications.InceptionResNetV2(weights='imagenet', input_shape=(299,299,3), include_top=False)\n",
    "#model.load_weights('/kaggle/input/inceptionresnetv2/inception_resent_v2_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "model.trainable=False\n",
    "\n",
    "x=model.output\n",
    "x=tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "x=tf.keras.layers.Dense(128,activation='relu')(x)\n",
    "x=tf.keras.layers.Dense(64,activation='relu')(x) \n",
    "outputs=tf.keras.layers.Dense(4,activation='sigmoid')(x) #final layer multilabel classifier\n",
    "\n",
    "model_multi=tf.keras.Model(inputs=model.input,outputs=outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b71d561-5fc8-4343-8bb4-8ad28384ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZER = tf.keras.optimizers.Adam(learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2401d4d-8e8f-44fd-90e5-d6ffb49e4d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_multi.compile(optimizer=OPTIMIZER, loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False), metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b65cb-070d-4b27-b49a-54b1366965ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.inception_resnet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766167a-c32a-40b2-9ece-f61452df2c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = preprocess_input(x_train)\n",
    "x_val = preprocess_input(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7580776a-0bb7-434c-ae3b-7465044d80d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135ad372-6918-4e31-8c56-a15e9eb65d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffb2e97-56eb-4e32-8806-3d63b5441f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49a7063-947c-4f4c-91dc-be1faee83687",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd62dc67-09a6-446b-9057-1e7c2351cb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehotencoder = OneHotEncoder()\n",
    "#reshape the 1-D array to 2-D as fit_transform expects 2-D and finally fit the object \n",
    "y_train = onehotencoder.fit_transform(y_train.reshape(-1,1)).toarray()\n",
    "y_val = onehotencoder.fit_transform(y_val.reshape(-1,1)).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac1ab4-bf77-46ce-b401-7b9a12957ebc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1088cfc-a22f-4e93-bcb4-c4adc39723c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7814cdf-5fce-4ecc-b011-f7b511be9e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b61529-0ef2-4180-99e4-48b516b971e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536348bf-f10e-4593-89ac-ac5a50c24240",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a664b07-3a94-4832-8df8-833ef254ec38",
   "metadata": {},
   "outputs": [],
   "source": [
    "history=model_multi.fit(x_train, \n",
    "                  y_train,\n",
    "                  batch_size=16, \n",
    "                  epochs=17,\n",
    "                  verbose=1,\n",
    "                  validation_data=(x_val, y_val)\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4b1e56-17a5-4b26-81fa-31390073e59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_multi.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3776b254-de66-43ed-8670-55d5666f1432",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca52eac-3f6a-496f-aff1-ba28b9fb4f8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
