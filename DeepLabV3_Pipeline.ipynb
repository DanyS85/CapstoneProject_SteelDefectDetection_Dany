{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecbb964b-ae03-4b7a-b6d7-26d0f099e0dc",
   "metadata": {},
   "source": [
    "## Using DeepLabV3 To The Steel Data\n",
    "\n",
    "Source:\n",
    "https://keras.io/examples/vision/deeplabv3_plus/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc7d18a2-d768-48ed-8617-8ed714e9f870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from scipy.io import loadmat\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "# Import required packages \n",
    "import image_modeling   # import image_modeling.py file\n",
    "import tensorflow_hub as hub\n",
    "import datetime\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset  # For custom data-sets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms.functional as TF\n",
    "from PIL import Image\n",
    "import glob\n",
    "import random\n",
    "import csv\n",
    "\n",
    "\n",
    "import pathlib\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "204df3a7-0e7f-4af0-87df-49f3f8fe3ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscellaneous\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfd9b205-ab98-4c91-8deb-ff51f35c2dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make some dark cell magic. Why not!\n",
    "from IPython.core.magic import register_cell_magic\n",
    "\n",
    "@register_cell_magic\n",
    "def write_and_run(line, cell):\n",
    "    argz = line.split()\n",
    "    file = argz[-1]\n",
    "    mode = 'w'\n",
    "    if len(argz) == 2 and argz[0] == '-a':\n",
    "        mode = 'a'\n",
    "        print(\"Appended to file \", file)\n",
    "    else:\n",
    "        print('Written to file:', file)\n",
    "    with open(file, mode) as f:\n",
    "        f.write(cell.format(**globals()))        \n",
    "    get_ipython().run_cell(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0c7d653-37c7-4dc9-9d19-75d7bc9f7e69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: %%write_and_run is a cell magic, but the cell body is empty.\n"
     ]
    }
   ],
   "source": [
    "%%write_and_run image_modeling.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4e51e40-e720-4ecd-85a0-f76355efd91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://thispointer.com/python-how-to-get-list-of-files-in-directory-and-sub-directories/\n",
    "def getListOfFiles(dirName):\n",
    "    # create a list of file and sub directories \n",
    "    # names in the given directory \n",
    "    listOfFile = os.listdir(dirName)\n",
    "    allFiles = list()\n",
    "    # Iterate over all the entries\n",
    "    for entry in listOfFile:\n",
    "        # Create full path\n",
    "        fullPath = os.path.join(dirName, entry)\n",
    "        # If entry is a directory then get the list of files in this directory \n",
    "        if os.path.isdir(fullPath):\n",
    "            allFiles = allFiles + getListOfFiles(fullPath)\n",
    "        else:\n",
    "            allFiles.append(fullPath)\n",
    "                \n",
    "    return allFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6bbc7b99-8cb0-4caf-a6a5-207b676f6787",
   "metadata": {},
   "outputs": [],
   "source": [
    "path= os.getcwd()\n",
    "\n",
    "\n",
    "train_path = path + '/Data/segmentation/train'\n",
    "train_mask_path = path + '/Data/segmentation/train_mask'\n",
    "test_path = path +'/Data/segmentation/test'\n",
    "test_mask_path =path + '/Data/segmentation/train'\n",
    "# Get the list of all files in directory tree at given path\n",
    "train_list = getListOfFiles(train_path)\n",
    "train_mask_list=getListOfFiles(train_mask_path)\n",
    "test_list = getListOfFiles(test_path)\n",
    "test_mask_list=getListOfFiles(test_mask_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a60e280c-3fc2-48de-b3d4-62b815fe0c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import variables from image_modelling.py file\n",
    "HEIGHT = image_modeling.HEIGHT\n",
    "WIDTH = image_modeling.WIDTH\n",
    "NCLASSES = image_modeling.NCLASSES\n",
    "CLASS_NAMES = image_modeling.CLASS_NAMES\n",
    "BATCH_SIZE = image_modeling.BATCH_SIZE\n",
    "TRAINING_SIZE = len(train_list)\n",
    "# Self coded\n",
    "go_trough_per_epoch = int(1)\n",
    "TRAINING_STEPS = (int(TRAINING_SIZE) // BATCH_SIZE) * go_trough_per_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70c1b908-d66e-4243-b7ab-ee6b0e534aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path_df = pd.DataFrame({'Path':train_list})\n",
    "train_path_df.to_csv('./Data/train_path.csv') \n",
    "\n",
    "train_mask_path_df = pd.DataFrame({'Path':train_mask_list})\n",
    "train_mask_path_df.to_csv('./Data/train_mask_path.csv')\n",
    "\n",
    "test_path_df = pd.DataFrame({'Path':test_list})\n",
    "test_path_df.to_csv('./Data/test_path.csv')\n",
    "\n",
    "test_mask_path_df = pd.DataFrame({'Path':test_mask_list})\n",
    "test_mask_path_df.to_csv('./Data/test_mask_path.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42005ca3-b0c4-4ce8-b898-7926278fbeae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n",
      "['1', '2', '3', '4']\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "# You can compare this output with the variables in the image_modelling.py file...\n",
    "print(HEIGHT)\n",
    "print(CLASS_NAMES)\n",
    "print(NCLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e9c75f7-aa9b-44ab-8342-3779c5d1420d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended to file  image_modeling.py\n"
     ]
    }
   ],
   "source": [
    "%%write_and_run -a image_modeling.py\n",
    "\n",
    "# We set some parameters for the model\n",
    "HEIGHT = 256 #image height\n",
    "WIDTH = 1600 #image width\n",
    "CHANNELS = 3 #image RGB channels\n",
    "CLASS_NAMES = ['1', '2', '3', '4']\n",
    "NCLASSES = len(CLASS_NAMES)\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER = 10 * BATCH_SIZE\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "VALIDATION_SIZE = 370\n",
    "VALIDATION_STEPS = VALIDATION_SIZE // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c92a7bc4-6fd6-436c-93b9-659d5ab47f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended to file  image_modeling.py\n"
     ]
    }
   ],
   "source": [
    "%%write_and_run -a image_modeling.py\n",
    "\n",
    "# Define the function that decodes in the images\n",
    "def decode_image(image, reshape_dim):\n",
    "    # JPEG is a compressed image format. So we want to \n",
    "    # convert this format to a numpy array we can compute with.\n",
    "    image = tf.image.decode_jpeg(image, channels=CHANNELS)\n",
    "    # 'decode_jpeg' returns a tensor of type uint8. We need for \n",
    "    # the model 32bit floats. Actually we want them to be in \n",
    "    # the [0,1] interval.\n",
    "    image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "    # Now we can resize to the desired size.\n",
    "    image = tf.image.resize(image, reshape_dim)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d4ca0fa-463e-4c5e-93ef-7d41c07f3c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/michael/neuefische/CapstoneProject_SteelDefectDetection/Data/segmentation/train/c1/eeffa4c49.jpg'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "602a758d-972f-47dc-ac99-e4f9cebb89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us test our decoding function.\n",
    "img = tf.io.read_file(train_list[0])    \n",
    "\n",
    "# TODO: take the function above and decode the image\n",
    "img = decode_image(img, [256,1600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cfd35463-bbd8-4d1f-81aa-b826e6b06f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended to file  image_modeling.py\n"
     ]
    }
   ],
   "source": [
    "%%write_and_run -a image_modeling.py\n",
    "\n",
    "# The train set actually gives only the paths to the training images.\n",
    "# We want to create a dataset of training images, so we need a \n",
    "# function that can handle this for us.\n",
    "def decode_dataset(data_row):\n",
    "    record_defaults = ['path', 'class']\n",
    "    filename, label_string = tf.io.decode_csv(data_row, record_defaults)\n",
    "    image_bytes = tf.io.read_file(filename=filename)\n",
    "    label = tf.math.equal(label_string, CLASS_NAMES)\n",
    "    return image_bytes, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5c7df6ab-1dd2-478c-8c3a-ba6c89c5f993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 21:03:32.002318: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'path'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.TextLineDataset('./Data/train_path.csv')\n",
    "it = iter(dataset)\n",
    "record_defaults = ['path','class'] # defines dtype\n",
    "# output dtype of decode_csv will be two strings. could have written ['chicken','egg'] with same outcome. But not e.g. [1,'class'].\n",
    "filename, label_string = tf.io.decode_csv(next(it), record_defaults)\n",
    "filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f0b7939-cc94-41be-aa19-4fe2d801bba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended to file  image_modeling.py\n"
     ]
    }
   ],
   "source": [
    "%%write_and_run -a image_modeling.py\n",
    "\n",
    "# Next we construct a function for pre-processing the images.\n",
    "def read_and_preprocess(image_bytes, label, augment_randomly=False):\n",
    "    if augment_randomly: \n",
    "        image = decode_image(image_bytes, [HEIGHT + 8, WIDTH + 8])\n",
    "        # TODO: Augment the image.\n",
    "        import random\n",
    "        offset_h = random.randint(0,8)\n",
    "        offset_w = random.randint(0,8)\n",
    "        image = image [offset_h:264-(8-offset_h),offset_w:1608-(8-offset_w)]\n",
    "    else:\n",
    "        image = decode_image(image_bytes, [HEIGHT, WIDTH])\n",
    "    return image, label\n",
    "\n",
    "def read_and_preprocess_with_augmentation(image_bytes, label): \n",
    "    return read_and_preprocess(image_bytes, label, augment_randomly=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fe50a71a-8ef1-4950-bd56-c46533260ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appended to file  image_modeling.py\n"
     ]
    }
   ],
   "source": [
    "%%write_and_run -a image_modeling.py\n",
    "\n",
    "# Now we can create the dataset.\n",
    "def load_dataset(file_of_filenames, batch_size, training=False):\n",
    "    # We create a TensorFlow Dataset from the list of files.\n",
    "    # This dataset does not load the data into memory, but instead\n",
    "    # pulls batches one after another.\n",
    "    dataset = tf.data.TextLineDataset(filenames=file_of_filenames).\\\n",
    "        map(decode_dataset)\n",
    "    \n",
    "    if training:\n",
    "        # TODO: Use augmentation here.\n",
    "        dataset = dataset.map(read_and_preprocess_with_augmentation).\\\n",
    "            shuffle(SHUFFLE_BUFFER).\\\n",
    "            repeat(count=None) # Infinite iterations\n",
    "    else: \n",
    "        # Evaluation or testing\n",
    "        dataset = dataset.map(read_and_preprocess).\\\n",
    "            repeat(count=1) # One iteration\n",
    "            \n",
    "    # The dataset will produce batches of BATCH_SIZE and will\n",
    "    # automatically prepare an optimized number of batches while the prior one is\n",
    "    # trained on.\n",
    "    return dataset.batch(batch_size).prefetch(buffer_size=AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a71d1f2c-ec7b-437c-9d53-9859da041830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 256, 1600, 3), (None, 4)), types: (tf.float32, tf.bool)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = tf.data.TextLineDataset(filenames='./Data/train_path.csv').\\\n",
    "        map(decode_dataset)\n",
    "\n",
    "dataset = dataset.map(read_and_preprocess).\\\n",
    "            repeat(count=1) # One iteration\n",
    "\n",
    "dataset.batch(32).prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "848fbda4-d946-4fb7-8564-88f65eb1ff40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 21:05:51.965674: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 0; No such file or directory\n",
      "2022-02-16 21:05:51.965696: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 3; No such file or directory\n",
      "2022-02-16 21:05:51.965712: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 1; No such file or directory\n",
      "2022-02-16 21:05:51.965744: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 7; No such file or directory\n",
      "2022-02-16 21:05:51.965769: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 6; No such file or directory\n",
      "2022-02-16 21:05:51.965784: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 2; No such file or directory\n",
      "2022-02-16 21:05:51.965848: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 5; No such file or directory\n",
      "2022-02-16 21:05:51.965915: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: path; No such file or directory\n",
      "2022-02-16 21:05:51.965955: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 4; No such file or directory\n",
      "2022-02-16 21:05:51.965982: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 8; No such file or directory\n",
      "2022-02-16 21:05:51.965999: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 12; No such file or directory\n",
      "2022-02-16 21:05:51.966003: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 11; No such file or directory\n",
      "2022-02-16 21:05:51.966040: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 13; No such file or directory\n",
      "2022-02-16 21:05:51.966052: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 15; No such file or directory\n",
      "2022-02-16 21:05:51.966058: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 10; No such file or directory\n",
      "2022-02-16 21:05:51.966075: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 14; No such file or directory\n",
      "2022-02-16 21:05:51.966095: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 16; No such file or directory\n",
      "2022-02-16 21:05:51.966199: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 9; No such file or directory\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "path; No such file or directory\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create an iterator that runs over the training dataset.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m it \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_data)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/neuefische/CapstoneProject_SteelDefectDetection/.venv/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:800\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    799\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/CapstoneProject_SteelDefectDetection/.venv/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:783\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 783\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/CapstoneProject_SteelDefectDetection/.venv/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:2845\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2843\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   2844\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2845\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   2847\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/CapstoneProject_SteelDefectDetection/.venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7107\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7106\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7107\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: path; No such file or directory\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "# Let us see, if data loading works as intended.\n",
    "#train_path = './Data/train_path.csv'\n",
    "train_data = load_dataset('./Data/train_path.csv', 1)\n",
    "# Create an iterator that runs over the training dataset.\n",
    "it = iter(train_data)\n",
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62cadd82-3be2-4e3a-95f3-e0b7d0741845",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 21:04:58.131455: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 0; No such file or directory\n",
      "2022-02-16 21:04:58.131514: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 3; No such file or directory\n",
      "2022-02-16 21:04:58.131548: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 5; No such file or directory\n",
      "2022-02-16 21:04:58.131588: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 4; No such file or directory\n",
      "2022-02-16 21:04:58.131615: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 1; No such file or directory\n",
      "2022-02-16 21:04:58.131653: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 6; No such file or directory\n",
      "2022-02-16 21:04:58.131677: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 2; No such file or directory\n",
      "2022-02-16 21:04:58.131810: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: path; No such file or directory\n",
      "2022-02-16 21:04:58.131870: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 7; No such file or directory\n",
      "2022-02-16 21:04:58.132020: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 8; No such file or directory\n",
      "2022-02-16 21:04:58.132093: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 13; No such file or directory\n",
      "2022-02-16 21:04:58.132148: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 9; No such file or directory\n",
      "2022-02-16 21:04:58.132164: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 15; No such file or directory\n",
      "2022-02-16 21:04:58.132211: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 12; No such file or directory\n",
      "2022-02-16 21:04:58.132226: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 10; No such file or directory\n",
      "2022-02-16 21:04:58.132882: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 14; No such file or directory\n",
      "2022-02-16 21:04:58.133020: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 16; No such file or directory\n",
      "2022-02-16 21:04:58.133077: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 17; No such file or directory\n",
      "2022-02-16 21:04:58.133325: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 11; No such file or directory\n",
      "2022-02-16 21:04:58.133520: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 18; No such file or directory\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "path; No such file or directory\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [38]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/neuefische/CapstoneProject_SteelDefectDetection/.venv/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:800\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    799\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/CapstoneProject_SteelDefectDetection/.venv/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:783\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 783\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/CapstoneProject_SteelDefectDetection/.venv/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:2845\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2843\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   2844\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2845\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   2847\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/CapstoneProject_SteelDefectDetection/.venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7107\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7106\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7107\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: path; No such file or directory\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2a0a5655-8974-4e26-b752-069aef95f1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-16 21:10:35.353762: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at whole_file_read_ops.cc:116 : NOT_FOUND: 17; No such file or directory\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "0; No such file or directory\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[0;32mIn [48]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Iterate and see the pictures and labels\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m img_batch, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m image \u001b[38;5;241m=\u001b[39m img_batch[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(image)\n",
      "File \u001b[0;32m~/neuefische/CapstoneProject_SteelDefectDetection/.venv/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:800\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    799\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/CapstoneProject_SteelDefectDetection/.venv/lib/python3.9/site-packages/tensorflow/python/data/ops/iterator_ops.py:783\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 783\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/CapstoneProject_SteelDefectDetection/.venv/lib/python3.9/site-packages/tensorflow/python/ops/gen_dataset_ops.py:2845\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   2843\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   2844\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 2845\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2846\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[1;32m   2847\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/neuefische/CapstoneProject_SteelDefectDetection/.venv/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7107\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7106\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7107\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: 0; No such file or directory\n\t [[{{node ReadFile}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "# Iterate and see the pictures and labels\n",
    "img_batch, labels = next(it)\n",
    "image = img_batch[0]\n",
    "plt.imshow(image)\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f60cc9-3a38-4efb-97dc-39dcff63f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolution_block(\n",
    "    block_input,\n",
    "    num_filters=256,\n",
    "    kernel_size=3,\n",
    "    dilation_rate=1,\n",
    "    padding=\"same\",\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        num_filters,\n",
    "        kernel_size=kernel_size,\n",
    "        dilation_rate=dilation_rate,\n",
    "        padding=\"same\",\n",
    "        use_bias=use_bias,\n",
    "        kernel_initializer=keras.initializers.HeNormal(),\n",
    "    )(block_input)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def DilatedSpatialPyramidPooling(dspp_input):\n",
    "    dims = dspp_input.shape\n",
    "    x = layers.AveragePooling2D(pool_size=(dims[-3], dims[-2]))(dspp_input)\n",
    "    x = convolution_block(x, kernel_size=1, use_bias=True)\n",
    "    out_pool = layers.UpSampling2D(\n",
    "        size=(dims[-3] // x.shape[1], dims[-2] // x.shape[2]), interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "\n",
    "    out_1 = convolution_block(dspp_input, kernel_size=1, dilation_rate=1)\n",
    "    out_6 = convolution_block(dspp_input, kernel_size=3, dilation_rate=6)\n",
    "    out_12 = convolution_block(dspp_input, kernel_size=3, dilation_rate=12)\n",
    "    out_18 = convolution_block(dspp_input, kernel_size=3, dilation_rate=18)\n",
    "\n",
    "    x = layers.Concatenate(axis=-1)([out_pool, out_1, out_6, out_12, out_18])\n",
    "    output = convolution_block(x, kernel_size=1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c17139-37d4-4c9e-9b21-fd9090c4a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def DeeplabV3Plus(image_size, num_classes):\n",
    "    model_input = keras.Input(shape=(image_size, image_size, 3))\n",
    "    resnet50 = keras.applications.ResNet50(\n",
    "        weights=\"imagenet\", include_top=False, input_tensor=model_input\n",
    "    )\n",
    "    x = resnet50.get_layer(\"conv4_block6_2_relu\").output\n",
    "    x = DilatedSpatialPyramidPooling(x)\n",
    "\n",
    "    input_a = layers.UpSampling2D(\n",
    "        size=(image_size // 4 // x.shape[1], image_size // 4 // x.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "    input_b = resnet50.get_layer(\"conv2_block3_2_relu\").output\n",
    "    input_b = convolution_block(input_b, num_filters=48, kernel_size=1)\n",
    "\n",
    "    x = layers.Concatenate(axis=-1)([input_a, input_b])\n",
    "    x = convolution_block(x)\n",
    "    x = convolution_block(x)\n",
    "    x = layers.UpSampling2D(\n",
    "        size=(image_size // x.shape[1], image_size // x.shape[2]),\n",
    "        interpolation=\"bilinear\",\n",
    "    )(x)\n",
    "    model_output = layers.Conv2D(num_classes, kernel_size=(1, 1), padding=\"same\")(x)\n",
    "    return keras.Model(inputs=model_input, outputs=model_output)\n",
    "\n",
    "NUM_CLASSES = 4\n",
    "IMAGE_SIZE = 128\n",
    "\n",
    "\n",
    "model = DeeplabV3Plus(image_size=IMAGE_SIZE, num_classes=NUM_CLASSES)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030af3e-8c97-4f5c-9a62-54e0ca1f77a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate and see the pictures and labels\n",
    "#train_img_x, train_mask_y =next(my_train)\n",
    "#test_img_x, test_mask_y =next(my_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639e92b2-9fd2-4341-9ec6-55db13f36919",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.01),\n",
    "    loss=loss,\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "history = model.fit(train, validation_data= test, epochs=2, verbose=1)\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.title(\"Training Loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"Validation Loss\")\n",
    "plt.ylabel(\"val_loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history[\"val_accuracy\"])\n",
    "plt.title(\"Validation Accuracy\")\n",
    "plt.ylabel(\"val_accuracy\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073a13b6-1fb2-4c1d-a632-2891c563b46c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab3e0c8-1db5-4a02-b963-bc3f34302cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
