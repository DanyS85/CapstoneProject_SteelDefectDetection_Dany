{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba8dfd61-a4d5-4b49-9c74-f203d7609d5f",
   "metadata": {},
   "source": [
    "---\n",
    "## Building a `.csv` File with all Available Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dcc098-2b33-4d46-9f2f-0d39ea286dd4",
   "metadata": {},
   "source": [
    "Tutorial for [writing csv in Python](https://www.pythontutorial.net/python-basics/python-write-csv-file/)\n",
    "\n",
    "Useful information on [pathlib](https://www.atqed.com/python-current-path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c1e60-fdcb-4c15-977e-ba9e97e132ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pathlib\n",
    "import IPython.display as display\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "# self-written scripts\n",
    "import sys\n",
    "sys.path.insert(0, 'Python_Scripts')\n",
    "\n",
    "import util\n",
    "import surf_hog_analysis\n",
    "import surf_handling\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cebcb8-42a4-48a8-b934-5d9ed1fd4198",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### File Path Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fef477-6aa3-4d35-99fe-bb4afac11dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "home_path = str(pathlib.Path.home())\n",
    "# get current working directory\n",
    "cwd = pathlib.Path.cwd()\n",
    "\n",
    "# build complete paths for `train_data` and `test_data`\n",
    "# use `.joinpath()` to ensure operating system conform paths\n",
    "train_data_dir = cwd.joinpath('data', 'train_images')\n",
    "test_data_dir = cwd.joinpath('data', 'test_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99395564-5543-44c6-b7f5-982819c133cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4900db4b-4ceb-4988-ac0d-6d280106a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of images in folder\n",
    "image_count = len(list(train_data_dir.glob('*.jpg')))\n",
    "print(\"We have\", image_count, \"training images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd67355-6d8d-452e-916e-6953b690c65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out first 2 elements via UNIX commands\n",
    "!head -3 data/train.csv > /tmp/input.csv \n",
    "!cat /tmp/input.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51231ac-9b1d-402d-bb85-09ab7cb79de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a few images\n",
    "images = list(train_data_dir.glob('*.jpg'))\n",
    "\n",
    "for image in images[:5]:\n",
    "    display.display(Image.open(str(image)))\n",
    "    print(image.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1901198f-25ad-44bb-ac49-453c5a863b55",
   "metadata": {},
   "source": [
    "For our complete csv-file we will first extract all `ImageIds` from `train.csv`. Since there are images with more than one defect, and, hence, more than 1 line in `train.csv`, we will concat the missing image IDs to `train.csv`. To obtain the missing IDs, we construct a complete list of all images, eliminate all lines with `ImageIds` from `train.csv` and then concatenate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2b1e0b-d75a-4e58-9ebb-233d8c16cc4f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Prepare train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435c9f20-9acc-4420-823f-dd464e2e54b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_defects = pd.read_csv('data/train.csv')\n",
    "# create image paths for \n",
    "defect_paths = df_defects.ImageId.apply(lambda x: train_data_dir.joinpath(x))\n",
    "# add column to the left of the data frame\n",
    "df_defects = pd.concat([pd.Series(defect_paths, name='FilePath'), df_defects], axis = 1)\n",
    "df_defects.FilePath[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374f6f4c-0e21-41b5-9a17-82cee1bcd6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate `ImageIds` for images with defect\n",
    "defect_ids = df_defects.ImageId.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161478a1-7acf-412d-b802-a45a2dc27bdb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Building the CSV-File\n",
    "\n",
    "Create a csv file with all image paths, the respective `ImageId` and an initialisation for `ClassId` and `EncodedPixels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea632125-7afd-47e8-97e0-d857c35d029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['FilePath', 'ImageId', 'ClassId', 'EncodedPixels']\n",
    "\n",
    "rows = []\n",
    "\n",
    "for image in images:\n",
    "    # `.as_posix()` returns the complete path\n",
    "    # `.name` returns the image name\n",
    "    # set `ClassId` and `EncodedPixels` to 0\n",
    "    rows.append([image.as_posix(), image.name, 0, '0'])\n",
    "    \n",
    "with open(train_data_dir.parent.joinpath('train_raw.csv'), # `.parent` returns the path up to the data directory\n",
    "          'w', \n",
    "          encoding = 'UTF8',\n",
    "          newline = '' # avoid blank lines between rows\n",
    "         ) as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(rows) # write row into file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe15de1-6337-4d97-bcb5-e6ad8573a271",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_csv('data/train_raw.csv')\n",
    "\n",
    "# get indices of `df_raw` for row dropping\n",
    "indices = []\n",
    "for idx, row in df_raw.iterrows():\n",
    "    if row.ImageId in defect_ids:\n",
    "        indices.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628aef38-c337-449e-a4f1-2123b9b17577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check wether all indices or defected images are caught\n",
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b116b7c4-f008-4b0a-aac0-897331d2a5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw.drop(indices, inplace=True)\n",
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61afc97-fd6f-44fa-abc5-d461e1b5123d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all rows of unclassified images to the defected images\n",
    "df_complete = pd.concat([df_defects, df_raw], axis=0, ignore_index=True)\n",
    "df_complete['Defect'] = df_complete.ClassId.apply(lambda x: 1 if x > 0 else 0)\n",
    "df_complete.to_csv('data/train_complete.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf970b3a-c4f7-40ff-a871-e25273b799df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eliminate unused csv file\n",
    "!rm -f data/train_raw.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164ba743-e9be-4ba6-a98f-8710774287d1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Build `dfs` for Single Defects and Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfa3586-3e31-4318-9334-5f456a3add42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Only execute once to create `.csv` file\"\"\"\n",
    "df = pd.read_csv('data/train_complete.csv')\n",
    "\n",
    "df = util.add_blackness_attributes(df.query('Defect==1'), 'train_images')\n",
    "\n",
    "util.isolate_single_defects(df)\n",
    "\n",
    "df.to_csv('data/train_single_defects_with_blackness.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b94f25-f108-4b5e-beaf-5aa19cb5d108",
   "metadata": {},
   "source": [
    "### Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1f0645-1cb3-4ab3-b40d-5c64a2c31cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import albumentations\n",
    "import cv2\n",
    "import time\n",
    "import random\n",
    "\n",
    "# self-written scripts\n",
    "import sys\n",
    "sys.path.insert(0, 'Python_Scripts')\n",
    "\n",
    "import data_preparation_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9751b7-9a76-4ca5-882c-246fbdef8ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_folder():\n",
    "    # prepare folder structure\n",
    "    try:\n",
    "        path = os.getcwd()\n",
    "        temp_path = path + \"/data/augmentations\"\n",
    "        os.mkdir(temp_path)\n",
    "    except:\n",
    "        print('Folder already exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e37f3aa-e87a-45b1-b391-253c706d81af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "augment = A.Compose([\n",
    "    #A.VerticalFlip(p=0.5),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    # A.OneOf([\n",
    "    #     A.ElasticTransform(alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03, p=0.5),\n",
    "    #     A.GridDistortion(p=0.5),\n",
    "    #     A.OpticalDistortion(distort_limit=2, shift_limit=0.5, p=1)                  \n",
    "    #     ], p=0.8),\n",
    "    A.CLAHE(p=0.8),\n",
    "    A.RandomBrightnessContrast(p=0.8),    \n",
    "    A.RandomGamma(p=0.8)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b8ebf-5f31-4e8a-a722-cca768aa7f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augement_images(image_ids, num_augmentations, class_id):\n",
    "    print(f'beginning augmentation for ClassId {class_id}...')\n",
    "    start = time.time()\n",
    "    \n",
    "    path = os.getcwd()\n",
    "    #path_suffix = 'c' + str(class_id) + '/'\n",
    "    \n",
    "    target_directory_image = '/data/augmentations/'\n",
    "    \n",
    "    aug_ids = []\n",
    "    class_ids = []\n",
    "    file_paths = []\n",
    "    \n",
    "    i = 1\n",
    "    \n",
    "    while i <= num_augmentations:\n",
    "        #print(i)\n",
    "        number = random.randint(0, len(image_ids) -1)\n",
    "        image_id = image_ids[number]\n",
    "        #print(image_id, mask_id)\n",
    "        \n",
    "        aug_ids.append('aug_' + str(i) + '_' + image_id)\n",
    "        class_ids.append(class_id)\n",
    "        file_paths.append(path + target_directory_image + image_id)\n",
    "        \n",
    "        original_image = cv2.imread('data/train_images/' + image_id)\n",
    "        #print(original_image)\n",
    "      \n",
    "        augmented = augment(image=original_image)\n",
    "        transformed_image = augmented['image']\n",
    "        #transformed_mask = augmented['mask']\n",
    "        \n",
    "        os.chdir(path + target_directory_image)\n",
    "        written = cv2.imwrite('aug_' + str(i) + '_' + image_id, transformed_image)\n",
    "        #print('image written:',written')\n",
    "\n",
    "        os.chdir(path)\n",
    "        \n",
    "        i += 1\n",
    "    \n",
    "    temp = pd.DataFrame(list(zip(file_paths,aug_ids, class_ids)), columns=['FilePath','ImageId','ClassId'])\n",
    "    \n",
    "    end = time.time()\n",
    "    print(f'augmented {num_augmentations} images of ClassId {class_id}')\n",
    "    print('time required for augmentation:', end - start)\n",
    "    print()\n",
    "    \n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4b1d8a-7cb0-408c-b2be-17c9861ca642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df_aug(df):\n",
    "    \n",
    "    make_folder()\n",
    "    \n",
    "    num_images_class_3 = df.groupby('ClassId').count().ImageId[3]\n",
    "    max_images = num_images_class_3\n",
    "\n",
    "    # create empty df\n",
    "    df_aug = pd.DataFrame(columns=['FilePath','ImageId','ClassId'])\n",
    "\n",
    "    for i in [1,2,3,4]:\n",
    "        image_ids = df.query('ClassId == @i').ImageId.values\n",
    "\n",
    "        temp = augement_images(image_ids=image_ids, num_augmentations=max_images, class_id=i)\n",
    "        df_aug=pd.concat([df_aug, temp], axis=0)\n",
    "\n",
    "    return df_aug.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c217c3-f291-454a-ab76-953811c1a09a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Augmentation for Train-Test-Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75785c17-eaf4-4eed-88c7-57f2bb590b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sd = pd.read_csv('data/train_single_defects_with_blackness.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d0357-df7c-4f71-91b3-2a88f8d3fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_sd.copy()\n",
    "y = X.pop('ClassId')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b5c8c-1584-477d-a25b-9a96a71dde31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame for train and test\n",
    "df_train = pd.concat([X_train, y_train], axis=1)\n",
    "df_test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02bc86-ba08-4062-8e83-ef291d8c382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Only execute ONCE\"\"\"\n",
    "# apply augmentation to train images\n",
    "df_train_aug = create_df_aug(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a62e81-731f-488d-8f16-7a62c56c86ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train-test-splits to .csv to feed them into the models\n",
    "df_train_aug.to_csv('data/train_set_augmented.csv', sep=',', index=False)\n",
    "df_test.to_csv('data/test_set_for_augmented.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f796b5e-a1a7-4ba1-8d3f-150d87587c7e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Histogramm of Oriented Gradients (HOG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d250b2-3a9c-4f1c-a320-60b585a69100",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create HoG for all images in `train_images`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a73463-683c-4974-89f4-ec4a5cf03a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current working directory\n",
    "cwd = pathlib.Path.cwd()\n",
    "train_data_dir = cwd.joinpath('data', 'train_images')\n",
    "\n",
    "train_images = list(train_data_dir.glob('*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181ee65f-a501-44df-8ba8-4cdbab292cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_images = []\n",
    "hog_features = []\n",
    "Image_Ids = []\n",
    "for image in train_images:\n",
    "    Image_Ids.append(image.name)\n",
    "    image = io.imread(\"data/train_images/\"+image.name)\n",
    "    resized_img = resize(image, (64,128))\n",
    "#    blur = cv.GaussianBlur(image,(5,5),0)\n",
    "    fd,hog_image = hog(resized_img, orientations=9, pixels_per_cell=(8,8),cells_per_block=(2,2),visualize=True,channel_axis=-1)\n",
    "    hog_images.append(hog_image)\n",
    "    hog_features.append(fd)\n",
    "  \n",
    "hog_features = np.array(hog_features)\n",
    "\n",
    "hog_features.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295b91d1-782d-4cbb-be5b-c96935dadfe9",
   "metadata": {},
   "source": [
    "Once, HOG-features are generated, we can build a data frame from it and save it as `.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe3272-b1ef-41e1-b8ec-f54ca62c2677",
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_features= pd.DataFrame(hog_features)\n",
    "\n",
    "Image_Ids= pd.DataFrame(Image_Ids)\n",
    "Image_Ids.rename(columns={0: 'ImageId'}, inplace=True)\n",
    "\n",
    "# put everything together\n",
    "hog_complete = pd.concat([hog_features, Image_Ids], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cd2a98-dcf0-4457-94ac-a6fcdef03689",
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_complete.to_csv('data/train_HOG.csv', sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f804dd8e-a880-4522-bde4-5d4126ed5240",
   "metadata": {},
   "source": [
    "### Create HoG for all images in `train_single_defects_augmented`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964ec9e9-ea96-4a3e-9cc4-80ecebb3e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current working directory\n",
    "cwd = pathlib.Path.cwd()\n",
    "train_data_dir2 = cwd.joinpath('data', 'augmentations')\n",
    "\n",
    "train_images2 = list(train_data_dir2.glob('*.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2381c7ed-1c5c-491a-b8b1-c27b0ebd2289",
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_images_augmented = []\n",
    "hog_features_augmented = []\n",
    "Image_Ids2 = []\n",
    "for image in train_images2:\n",
    "    Image_Ids2.append(image.name)\n",
    "    image = io.imread(\"data/augmentations/\"+image.name)\n",
    "    resized_img = resize(image, (64,128))\n",
    "    fd2,hog_image_augmented = hog(resized_img, orientations=9, pixels_per_cell=(8,8),cells_per_block=(2,2),visualize=True,channel_axis=-1)\n",
    "    hog_images_augmented.append(hog_image_augmented)\n",
    "    hog_features_augmented.append(fd2)\n",
    "  \n",
    "hog_features_augmented = np.array(hog_features_augmented)\n",
    "\n",
    "hog_features_augmented.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1807f32-fa8b-4fbb-aacc-d3e5e8b396bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_features_augmented= pd.DataFrame(hog_features_augmented)\n",
    "\n",
    "Image_Ids2= pd.DataFrame(Image_Ids2)\n",
    "Image_Ids2.rename(columns={0: 'ImageId'}, inplace=True)\n",
    "\n",
    "hog_complete_augmented = pd.concat([hog_features_augmented, Image_Ids2], axis=1, ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee034379-88c2-4ae2-9e64-5467fe5f0ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_complete_augmented.to_csv('data/train_HOG_augmented.csv', sep=',',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e600bce7-0b4f-429d-b0e1-005da3dc65d1",
   "metadata": {},
   "source": [
    "## with Gaussian blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a41f2d5-20ad-49ed-bcd1-d89589f936f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "hog_images_augmented_blur = []\n",
    "hog_features_augmented_blur = []\n",
    "Image_Ids2 = []\n",
    "for image in train_images2:\n",
    "    Image_Ids2.append(image.name)\n",
    "    image = io.imread(\"data/augmentations/\"+image.name)\n",
    "    resized_img = resize(image, (64,128))\n",
    "    blur = cv2.GaussianBlur(image,(5,5),0)\n",
    "    fd2,hog_image_augmented_blur = hog(blur, orientations=9, pixels_per_cell=(8,8),cells_per_block=(2,2),visualize=True,channel_axis=-1)\n",
    "    hog_images_augmented_blur.append(hog_image_augmented_blur)\n",
    "    hog_features_augmented_blur.append(fd2)\n",
    "  \n",
    "hog_features_augmented_blur = np.array(hog_features_augmented_blur)\n",
    "\n",
    "hog_features_augmented_blur.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a3864e-8eb0-4ac7-b8d5-3d1e88165e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_features_augmented_blur= pd.DataFrame(hog_features_augmented_blur)\n",
    "hog_features_augmented_blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77ecafd-43fb-4d82-b6cb-81a049e2f694",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image_Ids2= pd.DataFrame(Image_Ids2)\n",
    "Image_Ids2.rename(columns={0: 'ImageId'}, inplace=True)\n",
    "Image_Ids2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f071ec1-3009-46fe-8bcd-d14f61266590",
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_complete_augmented_blur = pd.concat([hog_features_augmented_blur, Image_Ids2], axis=1, ignore_index=False)\n",
    "hog_complete_augmented_blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4992788-7063-4842-b560-bdd986e02830",
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_complete_augmented_blur.to_csv('data/train_HOG_augmented_blur.csv', sep=',',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9aa288-7872-4e45-aacf-fbbb70ad9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in hog_images_augmented_blur[:5]:\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d83112-1a01-4e1d-bdda-fbe8ded965cc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## SURF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada28211-6d59-4d4d-82b9-702a99873489",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Processing time: ~3 minutes and 40 seconds.\n",
    "\"\"\"\n",
    "\n",
    "# get current working directory\n",
    "cwd = pathlib.Path.cwd()\n",
    "train_data_dir = cwd.joinpath('data', 'train_images')\n",
    "train_images = list(train_data_dir.glob('*.jpg'))\n",
    "\n",
    "# Create SURF object. You can specify params here or later.\n",
    "# Here I set Hessian Threshold to 400\n",
    "surf = cv2.xfeatures2d.SURF_create(400)\n",
    "\n",
    "temp = surf_handling.build_keypoints_from_list(train_images, surf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace25ccc-e9c7-40c7-b7b5-6a4233c97375",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have {temp.query('NumberKP < 50').count()[0]} keypoint vectors with less than 50 keypoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e8e0fe-fecb-4df6-9c3b-f6357520dbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.sort_values(by='NumberKP', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7c952b-1d60-4f57-8438-763be57aa72f",
   "metadata": {},
   "source": [
    "Adjust data frame and eliminate images that have more than 1 defect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b3a80f-f69f-4474-9cb5-cda48f8ff9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "util.isolate_single_defects(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc4e3d7-2154-41ed-ae11-bad3fff5e620",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Prepare data frame with (max) TOP50 Keypoints per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4aff7e-92b8-4bc1-9703-4d02b60f44f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Expected run-time: ~ 35 minutes\n",
    "\"\"\"\n",
    "# apply functions to data frame\n",
    "temp = surf_handling.add_keypoint_parameters(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0342bd0f-9b7b-4a8f-a69b-6c929bad4f0f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Save data frame for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a34ef32-c95c-4fff-b20f-8094380db5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.to_csv('data/train_surf.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1063fb38-b418-46d5-9838-94c7b4a6d04b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
